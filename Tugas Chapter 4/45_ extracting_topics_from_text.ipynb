{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am learning NLP, it is very interesting and exciting. it includes machine learning and deep learning',\n",
       " 'My father is a data scientist and he is nlp expert',\n",
       " 'My sister has good exposure into android development']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = \"I am learning NLP, it is very interesting and exciting. it includes machine learning and deep learning\"\n",
    "doc2 = \"My father is a data scientist and he is nlp expert\"\n",
    "doc3 = \"My sister has good exposure into android development\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\gabriel viceraira\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\gabriel viceraira\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\gabriel viceraira\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\gabriel viceraira\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim) (6.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\GABRIEL VICERAIRA\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\GABRIEL\n",
      "[nltk_data]     VICERAIRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['l',\n",
       "  'e',\n",
       "  'a',\n",
       "  'r',\n",
       "  'n',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  'n',\n",
       "  'l',\n",
       "  'p',\n",
       "  'i',\n",
       "  'n',\n",
       "  't',\n",
       "  'e',\n",
       "  'r',\n",
       "  'e',\n",
       "  's',\n",
       "  't',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  'e',\n",
       "  'x',\n",
       "  'c',\n",
       "  'i',\n",
       "  't',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  'i',\n",
       "  'n',\n",
       "  'c',\n",
       "  'l',\n",
       "  'u',\n",
       "  'd',\n",
       "  'e',\n",
       "  's',\n",
       "  'm',\n",
       "  'a',\n",
       "  'c',\n",
       "  'h',\n",
       "  'i',\n",
       "  'n',\n",
       "  'e',\n",
       "  'l',\n",
       "  'e',\n",
       "  'a',\n",
       "  'r',\n",
       "  'n',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  'd',\n",
       "  'e',\n",
       "  'e',\n",
       "  'p',\n",
       "  'l',\n",
       "  'e',\n",
       "  'a',\n",
       "  'r',\n",
       "  'n',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g'],\n",
       " ['f',\n",
       "  'a',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'r',\n",
       "  'd',\n",
       "  'a',\n",
       "  't',\n",
       "  'a',\n",
       "  's',\n",
       "  'c',\n",
       "  'i',\n",
       "  'e',\n",
       "  'n',\n",
       "  't',\n",
       "  'i',\n",
       "  's',\n",
       "  't',\n",
       "  'n',\n",
       "  'l',\n",
       "  'p',\n",
       "  'e',\n",
       "  'x',\n",
       "  'p',\n",
       "  'e',\n",
       "  'r',\n",
       "  't'],\n",
       " ['s',\n",
       "  'i',\n",
       "  's',\n",
       "  't',\n",
       "  'e',\n",
       "  'r',\n",
       "  'g',\n",
       "  'o',\n",
       "  'o',\n",
       "  'd',\n",
       "  'e',\n",
       "  'x',\n",
       "  'p',\n",
       "  'o',\n",
       "  's',\n",
       "  'u',\n",
       "  'r',\n",
       "  'e',\n",
       "  'a',\n",
       "  'n',\n",
       "  'd',\n",
       "  'r',\n",
       "  'o',\n",
       "  'i',\n",
       "  'd',\n",
       "  'd',\n",
       "  'e',\n",
       "  'v',\n",
       "  'e',\n",
       "  'l',\n",
       "  'o',\n",
       "  'p',\n",
       "  'm',\n",
       "  'e',\n",
       "  'n',\n",
       "  't']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install and import libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "\n",
    "# Text preprocessing as discussed in chapter 2\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \" \".join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 4),\n",
       "  (1, 3),\n",
       "  (2, 2),\n",
       "  (3, 10),\n",
       "  (4, 5),\n",
       "  (5, 1),\n",
       "  (6, 9),\n",
       "  (7, 5),\n",
       "  (8, 1),\n",
       "  (9, 12),\n",
       "  (10, 2),\n",
       "  (11, 4),\n",
       "  (12, 2),\n",
       "  (13, 3),\n",
       "  (14, 1),\n",
       "  (15, 1)],\n",
       " [(0, 3),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 4),\n",
       "  (5, 1),\n",
       "  (6, 2),\n",
       "  (7, 1),\n",
       "  (9, 2),\n",
       "  (10, 2),\n",
       "  (11, 2),\n",
       "  (12, 2),\n",
       "  (13, 5),\n",
       "  (15, 1),\n",
       "  (16, 1)],\n",
       " [(0, 1),\n",
       "  (2, 4),\n",
       "  (3, 6),\n",
       "  (4, 1),\n",
       "  (6, 2),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 2),\n",
       "  (10, 2),\n",
       "  (11, 3),\n",
       "  (12, 3),\n",
       "  (13, 2),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (17, 5),\n",
       "  (18, 1)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting a list of documents (corpus) into Document-Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.323*\"o\" + 0.168*\"d\" + 0.080*\"v\" + 0.046*\"s\" + 0.044*\"u\" + 0.043*\"m\" + 0.027*\"r\" + 0.026*\"p\" + 0.026*\"x\" + 0.025*\"e\"'), (1, '0.053*\"f\" + 0.053*\"s\" + 0.053*\"x\" + 0.053*\"t\" + 0.053*\"p\" + 0.053*\"d\" + 0.053*\"r\" + 0.053*\"h\" + 0.053*\"e\" + 0.053*\"m\"'), (2, '0.162*\"e\" + 0.130*\"n\" + 0.106*\"i\" + 0.082*\"t\" + 0.074*\"r\" + 0.066*\"a\" + 0.058*\"l\" + 0.055*\"s\" + 0.050*\"g\" + 0.050*\"p\"')]\n"
     ]
    }
   ],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix for 3 topics.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word =\n",
    "dictionary, passes=50)\n",
    "\n",
    "# Results\n",
    "print(ldamodel.print_topics())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
